{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SwZLEhG2zmi"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext scikit-learn pandas numpy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LcEl_Tdn2-Gi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'negative', 'positive'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading GloVe: 400000it [00:07, 56528.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Embedding matrix prepared.\n",
            "Padded reviews shape: torch.Size([50000, 100])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'D:\\projects\\python\\sentiment-analysis\\sentiment analysis\\cleaned_imdb_reviews.csv')  # Replace with your dataset path\n",
        "\n",
        "reviews = data['review'].values\n",
        "sentiments = data['sentiment'].values\n",
        "print(set(sentiments))\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "glove_path = r'D:\\projects\\python\\sentiment-analysis\\sentiment analysis\\GloVe\\glove.6B\\glove.6B.100d.txt'  # Replace with your GloVe file path\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in tqdm(f, desc=\"Loading GloVe\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# Tokenize text and create vocabulary\n",
        "word_to_index = defaultdict(lambda: len(word_to_index))  # Assign unique index to each word\n",
        "word_to_index['<PAD>'] = 0  # Padding token\n",
        "word_to_index['<UNK>'] = 1  # Unknown token\n",
        "\n",
        "tokenized_reviews = []\n",
        "for review in reviews:\n",
        "    tokens = review.split()  # Simple whitespace tokenization\n",
        "    tokenized_reviews.append([word_to_index[token] for token in tokens])\n",
        "\n",
        "# Prepare embedding matrix\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, index in word_to_index.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[index] = torch.tensor(embeddings_index[word], dtype=torch.float)\n",
        "    else:\n",
        "        embedding_matrix[index] = torch.randn(embedding_dim)  # Random initialization for unknown words\n",
        "\n",
        "print(\"Embedding matrix prepared.\")\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = 100\n",
        "padded_reviews = pad_sequence(\n",
        "    [torch.tensor(seq[:max_sequence_length]) for seq in tokenized_reviews],\n",
        "    batch_first=True,\n",
        "    padding_value=word_to_index['<PAD>']\n",
        ")\n",
        "\n",
        "print(f\"Padded reviews shape: {padded_reviews.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4liCj9yBC9Kq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LSTMWithAttention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(LSTMWithAttention, self).__init__()\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size=embedding_matrix.size(1),\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=1,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Attention Layer\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def attention(self, lstm_out):\n",
        "        \"\"\"\n",
        "        Attention mechanism to compute attention scores and weight LSTM outputs.\n",
        "        \"\"\"\n",
        "        attn_weights = F.softmax(self.attn(lstm_out), dim=1)\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        attn_output = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the embedded representation\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pass through LSTM layer\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attn_out = self.attention(lstm_out)\n",
        "\n",
        "        # Pass through dropout\n",
        "        out = self.dropout(attn_out)\n",
        "\n",
        "        # Pass through fully connected layer for classification\n",
        "        output = self.fc(out)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KjLGgTKD6bs2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMWithAttention(\n",
              "  (embedding): Embedding(438731, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
              "  (attn): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 100  # GloVe embedding dimension\n",
        "hidden_dim = 128  # LSTM hidden size\n",
        "output_dim = 2  # Sentiment (positive or negative)\n",
        "dropout = 0.5  # Dropout rate\n",
        "batch_size = 64  # Batch size\n",
        "lr = 1e-3  # Learning rate\n",
        "\n",
        "# Initialize model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Training loop\n",
        "model = LSTMWithAttention(embedding_matrix, hidden_dim=128, output_dim=2)  # Example values\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M_29a7gD6kmS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming the sentiment values are strings like 'positive' and 'negative'\n",
        "sentiment_map = {'positive': 1, 'negative': 0}\n",
        "sentiments = data['sentiment'].map(sentiment_map).values\n",
        "\n",
        "# Convert data to tensors\n",
        "padded_reviews_tensor = padded_reviews  # Already padded\n",
        "sentiments_tensor = torch.tensor(sentiments, dtype=torch.long)  # Sentiment labels\n",
        "\n",
        "# Create a TensorDataset and DataLoader\n",
        "dataset = TensorDataset(padded_reviews_tensor, sentiments_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y8fOFh027Etj"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # For binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "66lll1c97ZoO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.6937, Accuracy: 50.12%\n",
            "Epoch 2/5, Loss: 0.6936, Accuracy: 50.29%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32md:\\projects\\python\\sentiment-analysis\\myenv\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\projects\\python\\sentiment-analysis\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\projects\\python\\sentiment-analysis\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backpropagation\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct_preds += (preds == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = correct_preds / total_preds * 100\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulE6DfDm9kga"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU and store\n",
        "        all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and store\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"])\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM3drt1B-dV-"
      },
      "outputs": [],
      "source": [
        "# Save the model's state dict (recommended)\n",
        "torch.save(model.state_dict(), 'lstm_attention_model.pth')\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(model, 'lstm_attention_full_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b0YVk2N-uFo"
      },
      "outputs": [],
      "source": [
        "# Initialize the model again with the same architecture\n",
        "model = LSTMWithAttention(embedding_matrix, hidden_dim=128, output_dim=2)  # Define your model architecture\n",
        "\n",
        "# Load the state dict into the model\n",
        "model.load_state_dict(torch.load('lstm_attention_model.pth'))\n",
        "model.to(device)  # Move to GPU if necessary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQZej-Hu_Jms"
      },
      "outputs": [],
      "source": [
        "def preprocess_review(review, word_to_index, max_length=100):\n",
        "    # Tokenize the review\n",
        "    tokens = review.split()  # Simple whitespace tokenization\n",
        "    tokenized = [word_to_index[token] if token in word_to_index else word_to_index['<UNK>'] for token in tokens]\n",
        "\n",
        "    # Pad the review\n",
        "    padded_review = torch.tensor(tokenized[:max_length])  # Ensure review is no longer than max_length\n",
        "    padded_review = F.pad(padded_review, (0, max_length - len(padded_review)), value=word_to_index['<PAD>'])\n",
        "\n",
        "    return padded_review.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Example custom review\n",
        "custom_review = \"the movie was not good\"\n",
        "processed_review = preprocess_review(custom_review, word_to_index)\n",
        "\n",
        "# Inference on the custom review\n",
        "model.eval()\n",
        "processed_review = processed_review.to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(processed_review)\n",
        "    predicted_class = torch.argmax(output, dim=1)\n",
        "    print(\"Predicted Class:\", \"Positive\" if predicted_class.item() == 1 else \"Negative\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgLpzWYA_tDO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_attention(idx, model, review_tensor):\n",
        "    # Get the LSTM outputs\n",
        "    embedded = model.embedding(review_tensor)\n",
        "    lstm_out, (h_n, c_n) = model.lstm(embedded)\n",
        "\n",
        "    # Get the attention weights\n",
        "    attn_weights = F.softmax(model.attn(lstm_out), dim=1)\n",
        "    attn_weights = attn_weights.squeeze().detach().cpu().numpy()  # Detach and move to CPU before converting to numpy\n",
        "\n",
        "    # Get the tokenized words (for visualization)\n",
        "    review_tokens = [\n",
        "        list(word_to_index.keys())[list(word_to_index.values()).index(int(word))]\n",
        "        for word in review_tensor.squeeze().cpu().numpy()\n",
        "    ]\n",
        "\n",
        "    # Add padding token representation explicitly for display purposes\n",
        "    review_tokens = [\n",
        "        token if token != '<PAD>' else f'<PAD> (index {word_to_index[\"<PAD>\"]})'\n",
        "        for token in review_tokens\n",
        "    ]\n",
        "\n",
        "    # Print each token and its corresponding attention weight\n",
        "    for token, attn in zip(review_tokens, attn_weights):\n",
        "        print(f\"Token: {token}, Attention Weight: {attn}\")\n",
        "\n",
        "    # Plotting the attention weights\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(review_tokens, attn_weights)\n",
        "    plt.xlabel('Attention Weight')\n",
        "    plt.title(f'Attention Weights for Review {idx}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3iAPbab_2b6"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for the custom review\n",
        "visualize_attention(0, model, processed_review)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
